{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler"
      ],
      "metadata": {
        "id": "43U2ua2honHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b6d7aXsU6BY"
      },
      "outputs": [],
      "source": [
        "class SizeDataset(Dataset):\n",
        "    def __init__(self, directory):\n",
        "        self.directory = directory\n",
        "        self.files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = os.path.join(self.directory, self.files[idx])\n",
        "        data = np.loadtxt(file_path).flatten()\n",
        "        return torch.tensor(data, dtype=torch.float32)\n",
        "\n",
        "human_dataset = SizeDataset(\"/content/human_sizes\")\n",
        "manga_dataset = SizeDataset(\"/content/manga_sizes\")\n",
        "\n",
        "human_dataloader = DataLoader(human_dataset, batch_size=5, shuffle=True)\n",
        "manga_dataloader = DataLoader(manga_dataset, batch_size=5, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(4, 48),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(48, 96),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(96, 48),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(48, 4)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(4, 48),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(48, 24),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(24, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "45FwWMxDVNPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_H2M = Generator()\n",
        "G_M2H = Generator()\n",
        "D_H = Discriminator()\n",
        "D_M = Discriminator()"
      ],
      "metadata": {
        "id": "B4KQ-3vy4zLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function and optimizers\n",
        "criterion_GAN = nn.BCELoss()\n",
        "criterion_cycle = nn.L1Loss()\n",
        "lr = 0.0002\n",
        "\n",
        "optimizer_G_H2M = optim.Adam(G_H2M.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "optimizer_G_M2H = optim.Adam(G_M2H.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "optimizer_D_H = optim.Adam(D_H.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "optimizer_D_M = optim.Adam(D_M.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "# Labels for real and fake data\n",
        "real_label = torch.ones((5, 1))\n",
        "fake_label = torch.zeros((5, 1))\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 500\n",
        "for epoch in range(num_epochs):\n",
        "    for (human_sizes, manga_sizes) in zip(human_dataloader, manga_dataloader):\n",
        "\n",
        "        # Train Generators\n",
        "        optimizer_G_H2M.zero_grad()\n",
        "        optimizer_G_M2H.zero_grad()\n",
        "\n",
        "        # GAN loss\n",
        "        fake_manga = G_H2M(human_sizes)\n",
        "        pred_fake = D_M(fake_manga)\n",
        "        loss_GAN_H2M = criterion_GAN(pred_fake, real_label)\n",
        "\n",
        "        fake_human = G_M2H(manga_sizes)\n",
        "        pred_fake = D_H(fake_human)\n",
        "        loss_GAN_M2H = criterion_GAN(pred_fake, real_label)\n",
        "\n",
        "        # Cycle consistency loss\n",
        "        recovered_human = G_M2H(fake_manga)\n",
        "        loss_cycle_H2M = criterion_cycle(recovered_human, human_sizes)\n",
        "\n",
        "        recovered_manga = G_H2M(fake_human)\n",
        "        loss_cycle_M2H = criterion_cycle(recovered_manga, manga_sizes)\n",
        "\n",
        "        # Total generator loss\n",
        "        loss_G = loss_GAN_H2M + loss_GAN_M2H + 10.0 * (loss_cycle_H2M + loss_cycle_M2H)\n",
        "        loss_G.backward()\n",
        "\n",
        "        optimizer_G_H2M.step()\n",
        "        optimizer_G_M2H.step()\n",
        "\n",
        "        # Train Discriminator H\n",
        "        optimizer_D_H.zero_grad()\n",
        "\n",
        "        # Real loss\n",
        "        pred_real = D_H(human_sizes)\n",
        "        loss_D_real = criterion_GAN(pred_real, real_label)\n",
        "\n",
        "        # Fake loss\n",
        "        pred_fake = D_H(fake_human.detach())\n",
        "        loss_D_fake = criterion_GAN(pred_fake, fake_label)\n",
        "\n",
        "        # Total loss\n",
        "        loss_D_H = (loss_D_real + loss_D_fake) * 0.5\n",
        "        loss_D_H.backward()\n",
        "        optimizer_D_H.step()\n",
        "\n",
        "        # Train Discriminator M\n",
        "        optimizer_D_M.zero_grad()\n",
        "\n",
        "        # Real loss\n",
        "        pred_real = D_M(manga_sizes)\n",
        "        loss_D_real = criterion_GAN(pred_real, real_label)\n",
        "\n",
        "        # Fake loss\n",
        "        pred_fake = D_M(fake_manga.detach())\n",
        "        loss_D_fake = criterion_GAN(pred_fake, fake_label)\n",
        "\n",
        "        # Total loss\n",
        "        loss_D_M = (loss_D_real + loss_D_fake) * 0.5\n",
        "        loss_D_M.backward()\n",
        "        optimizer_D_M.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}] Loss G: {loss_G.item()} Loss D_H: {loss_D_H.item()} Loss D_M: {loss_D_M.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxyqaBSBVQpG",
        "outputId": "4dc4f711-010f-4661-f36a-a7c6026321b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/500] Loss G: 4.3050432205200195 Loss D_H: 0.6955143213272095 Loss D_M: 0.690280556678772\n",
            "Epoch [2/500] Loss G: 3.6487743854522705 Loss D_H: 0.6940141916275024 Loss D_M: 0.6901440024375916\n",
            "Epoch [3/500] Loss G: 2.65667986869812 Loss D_H: 0.6927852630615234 Loss D_M: 0.6914514303207397\n",
            "Epoch [4/500] Loss G: 2.206876754760742 Loss D_H: 0.6925321817398071 Loss D_M: 0.6925336122512817\n",
            "Epoch [5/500] Loss G: 1.7412374019622803 Loss D_H: 0.6933138966560364 Loss D_M: 0.6932134628295898\n",
            "Epoch [6/500] Loss G: 1.7538437843322754 Loss D_H: 0.6936435699462891 Loss D_M: 0.6933724880218506\n",
            "Epoch [7/500] Loss G: 1.824974536895752 Loss D_H: 0.6934335231781006 Loss D_M: 0.692913830280304\n",
            "Epoch [8/500] Loss G: 1.7642637491226196 Loss D_H: 0.6931970119476318 Loss D_M: 0.6924493312835693\n",
            "Epoch [9/500] Loss G: 1.7392487525939941 Loss D_H: 0.6932030916213989 Loss D_M: 0.6926558017730713\n",
            "Epoch [10/500] Loss G: 1.76816725730896 Loss D_H: 0.6930494904518127 Loss D_M: 0.6933069229125977\n",
            "Epoch [11/500] Loss G: 1.7666369676589966 Loss D_H: 0.6931494474411011 Loss D_M: 0.6928873062133789\n",
            "Epoch [12/500] Loss G: 1.7109313011169434 Loss D_H: 0.6930538415908813 Loss D_M: 0.6923353672027588\n",
            "Epoch [13/500] Loss G: 1.7561285495758057 Loss D_H: 0.6930573582649231 Loss D_M: 0.6927233338356018\n",
            "Epoch [14/500] Loss G: 1.7848104238510132 Loss D_H: 0.6930669546127319 Loss D_M: 0.6919776201248169\n",
            "Epoch [15/500] Loss G: 1.7607030868530273 Loss D_H: 0.693115234375 Loss D_M: 0.6928706765174866\n",
            "Epoch [16/500] Loss G: 1.76674485206604 Loss D_H: 0.693139910697937 Loss D_M: 0.6930701732635498\n",
            "Epoch [17/500] Loss G: 1.6863327026367188 Loss D_H: 0.6931270360946655 Loss D_M: 0.6925642490386963\n",
            "Epoch [18/500] Loss G: 1.7983516454696655 Loss D_H: 0.693090558052063 Loss D_M: 0.6921142935752869\n",
            "Epoch [19/500] Loss G: 1.6585195064544678 Loss D_H: 0.6930543184280396 Loss D_M: 0.6927002668380737\n",
            "Epoch [20/500] Loss G: 1.6686865091323853 Loss D_H: 0.6930170655250549 Loss D_M: 0.6927169561386108\n",
            "Epoch [21/500] Loss G: 1.6311780214309692 Loss D_H: 0.6930134296417236 Loss D_M: 0.6928466558456421\n",
            "Epoch [22/500] Loss G: 1.706764578819275 Loss D_H: 0.6931016445159912 Loss D_M: 0.6923907995223999\n",
            "Epoch [23/500] Loss G: 1.7682440280914307 Loss D_H: 0.6931543350219727 Loss D_M: 0.6922369599342346\n",
            "Epoch [24/500] Loss G: 1.641994833946228 Loss D_H: 0.6930948495864868 Loss D_M: 0.6926849484443665\n",
            "Epoch [25/500] Loss G: 1.7371418476104736 Loss D_H: 0.6930472254753113 Loss D_M: 0.6917790174484253\n",
            "Epoch [26/500] Loss G: 1.6943703889846802 Loss D_H: 0.6930298209190369 Loss D_M: 0.692988395690918\n",
            "Epoch [27/500] Loss G: 1.9941391944885254 Loss D_H: 0.6930786371231079 Loss D_M: 0.6901964545249939\n",
            "Epoch [28/500] Loss G: 1.6993972063064575 Loss D_H: 0.6929864883422852 Loss D_M: 0.6922453045845032\n",
            "Epoch [29/500] Loss G: 1.75031578540802 Loss D_H: 0.6929904222488403 Loss D_M: 0.6921696662902832\n",
            "Epoch [30/500] Loss G: 1.753132700920105 Loss D_H: 0.6929416656494141 Loss D_M: 0.6917203664779663\n",
            "Epoch [31/500] Loss G: 1.7227797508239746 Loss D_H: 0.6930180788040161 Loss D_M: 0.691718339920044\n",
            "Epoch [32/500] Loss G: 1.809800386428833 Loss D_H: 0.6930192708969116 Loss D_M: 0.6921145915985107\n",
            "Epoch [33/500] Loss G: 1.7406597137451172 Loss D_H: 0.692987322807312 Loss D_M: 0.6918530464172363\n",
            "Epoch [34/500] Loss G: 1.781314730644226 Loss D_H: 0.6930651664733887 Loss D_M: 0.6920784711837769\n",
            "Epoch [35/500] Loss G: 1.7245159149169922 Loss D_H: 0.6930371522903442 Loss D_M: 0.6923036575317383\n",
            "Epoch [36/500] Loss G: 1.7796354293823242 Loss D_H: 0.693111777305603 Loss D_M: 0.6919986009597778\n",
            "Epoch [37/500] Loss G: 1.7729198932647705 Loss D_H: 0.6930240988731384 Loss D_M: 0.6916811466217041\n",
            "Epoch [38/500] Loss G: 1.661190152168274 Loss D_H: 0.693109393119812 Loss D_M: 0.6917417049407959\n",
            "Epoch [39/500] Loss G: 1.7051610946655273 Loss D_H: 0.6930350065231323 Loss D_M: 0.6918708086013794\n",
            "Epoch [40/500] Loss G: 1.741523265838623 Loss D_H: 0.6929926872253418 Loss D_M: 0.6914815902709961\n",
            "Epoch [41/500] Loss G: 1.7960901260375977 Loss D_H: 0.6930029392242432 Loss D_M: 0.691356360912323\n",
            "Epoch [42/500] Loss G: 1.7937750816345215 Loss D_H: 0.6929426193237305 Loss D_M: 0.6913787126541138\n",
            "Epoch [43/500] Loss G: 1.8723394870758057 Loss D_H: 0.6929411888122559 Loss D_M: 0.6907577514648438\n",
            "Epoch [44/500] Loss G: 1.732509732246399 Loss D_H: 0.6930699944496155 Loss D_M: 0.6923531293869019\n",
            "Epoch [45/500] Loss G: 1.7699646949768066 Loss D_H: 0.6929143071174622 Loss D_M: 0.6917188167572021\n",
            "Epoch [46/500] Loss G: 1.7689218521118164 Loss D_H: 0.6930465698242188 Loss D_M: 0.6913206577301025\n",
            "Epoch [47/500] Loss G: 1.8004481792449951 Loss D_H: 0.6930731534957886 Loss D_M: 0.6912775635719299\n",
            "Epoch [48/500] Loss G: 1.743343472480774 Loss D_H: 0.6929168701171875 Loss D_M: 0.6918741464614868\n",
            "Epoch [49/500] Loss G: 1.7615325450897217 Loss D_H: 0.6928865909576416 Loss D_M: 0.6910840272903442\n",
            "Epoch [50/500] Loss G: 1.764253854751587 Loss D_H: 0.6929184198379517 Loss D_M: 0.6908034086227417\n",
            "Epoch [51/500] Loss G: 1.7419052124023438 Loss D_H: 0.6929540038108826 Loss D_M: 0.6921907067298889\n",
            "Epoch [52/500] Loss G: 1.7178363800048828 Loss D_H: 0.6928898096084595 Loss D_M: 0.6897600889205933\n",
            "Epoch [53/500] Loss G: 1.7556085586547852 Loss D_H: 0.6930636167526245 Loss D_M: 0.6908978819847107\n",
            "Epoch [54/500] Loss G: 1.839281439781189 Loss D_H: 0.6929760575294495 Loss D_M: 0.6897425055503845\n",
            "Epoch [55/500] Loss G: 1.7283121347427368 Loss D_H: 0.6929738521575928 Loss D_M: 0.690430760383606\n",
            "Epoch [56/500] Loss G: 1.7571624517440796 Loss D_H: 0.6927270293235779 Loss D_M: 0.6916307210922241\n",
            "Epoch [57/500] Loss G: 1.715583086013794 Loss D_H: 0.6930732727050781 Loss D_M: 0.6902269124984741\n",
            "Epoch [58/500] Loss G: 1.746009111404419 Loss D_H: 0.6930245757102966 Loss D_M: 0.6916791796684265\n",
            "Epoch [59/500] Loss G: 1.7254061698913574 Loss D_H: 0.6929571032524109 Loss D_M: 0.6909025311470032\n",
            "Epoch [60/500] Loss G: 1.6967394351959229 Loss D_H: 0.6929773688316345 Loss D_M: 0.6905417442321777\n",
            "Epoch [61/500] Loss G: 1.8249244689941406 Loss D_H: 0.6928346157073975 Loss D_M: 0.6887879371643066\n",
            "Epoch [62/500] Loss G: 1.7448768615722656 Loss D_H: 0.6927648186683655 Loss D_M: 0.6900531053543091\n",
            "Epoch [63/500] Loss G: 1.711334228515625 Loss D_H: 0.692895770072937 Loss D_M: 0.6909307241439819\n",
            "Epoch [64/500] Loss G: 2.0258655548095703 Loss D_H: 0.6932067275047302 Loss D_M: 0.6807470321655273\n",
            "Epoch [65/500] Loss G: 1.7732796669006348 Loss D_H: 0.6930176019668579 Loss D_M: 0.6858292818069458\n",
            "Epoch [66/500] Loss G: 1.7243473529815674 Loss D_H: 0.6928633451461792 Loss D_M: 0.6886910200119019\n",
            "Epoch [67/500] Loss G: 1.671937346458435 Loss D_H: 0.6928719878196716 Loss D_M: 0.6904811859130859\n",
            "Epoch [68/500] Loss G: 1.7400821447372437 Loss D_H: 0.6929285526275635 Loss D_M: 0.688873827457428\n",
            "Epoch [69/500] Loss G: 1.8131483793258667 Loss D_H: 0.6926946640014648 Loss D_M: 0.6887233257293701\n",
            "Epoch [70/500] Loss G: 1.8143837451934814 Loss D_H: 0.6923545002937317 Loss D_M: 0.6899973154067993\n",
            "Epoch [71/500] Loss G: 1.9209632873535156 Loss D_H: 0.6929450035095215 Loss D_M: 0.6746792793273926\n",
            "Epoch [72/500] Loss G: 1.8031337261199951 Loss D_H: 0.6927368640899658 Loss D_M: 0.6894490718841553\n",
            "Epoch [73/500] Loss G: 1.651615858078003 Loss D_H: 0.6927217245101929 Loss D_M: 0.6911690831184387\n",
            "Epoch [74/500] Loss G: 1.7453986406326294 Loss D_H: 0.6931254863739014 Loss D_M: 0.6864532828330994\n",
            "Epoch [75/500] Loss G: 1.7181328535079956 Loss D_H: 0.6928951740264893 Loss D_M: 0.686516284942627\n",
            "Epoch [76/500] Loss G: 1.7665438652038574 Loss D_H: 0.6929214000701904 Loss D_M: 0.6869381666183472\n",
            "Epoch [77/500] Loss G: 1.9035179615020752 Loss D_H: 0.6926822662353516 Loss D_M: 0.674392580986023\n",
            "Epoch [78/500] Loss G: 1.778310775756836 Loss D_H: 0.6929227113723755 Loss D_M: 0.6850314140319824\n",
            "Epoch [79/500] Loss G: 1.8483275175094604 Loss D_H: 0.6927142143249512 Loss D_M: 0.6845628619194031\n",
            "Epoch [80/500] Loss G: 1.7390025854110718 Loss D_H: 0.6923527121543884 Loss D_M: 0.686355710029602\n",
            "Epoch [81/500] Loss G: 1.716227650642395 Loss D_H: 0.692865788936615 Loss D_M: 0.6884391903877258\n",
            "Epoch [82/500] Loss G: 1.6443367004394531 Loss D_H: 0.6927019357681274 Loss D_M: 0.6862495541572571\n",
            "Epoch [83/500] Loss G: 1.7865430116653442 Loss D_H: 0.6927740573883057 Loss D_M: 0.6860620975494385\n",
            "Epoch [84/500] Loss G: 1.6890860795974731 Loss D_H: 0.6930188536643982 Loss D_M: 0.6869494318962097\n",
            "Epoch [85/500] Loss G: 1.8521137237548828 Loss D_H: 0.6925160884857178 Loss D_M: 0.6819863319396973\n",
            "Epoch [86/500] Loss G: 1.7223325967788696 Loss D_H: 0.6928955316543579 Loss D_M: 0.6874302625656128\n",
            "Epoch [87/500] Loss G: 1.7160742282867432 Loss D_H: 0.6928237676620483 Loss D_M: 0.6838990449905396\n",
            "Epoch [88/500] Loss G: 1.7388205528259277 Loss D_H: 0.6927860975265503 Loss D_M: 0.6878920793533325\n",
            "Epoch [89/500] Loss G: 1.9211604595184326 Loss D_H: 0.6928983926773071 Loss D_M: 0.6675304174423218\n",
            "Epoch [90/500] Loss G: 1.724292278289795 Loss D_H: 0.6928753852844238 Loss D_M: 0.682766318321228\n",
            "Epoch [91/500] Loss G: 1.9350258111953735 Loss D_H: 0.6928986310958862 Loss D_M: 0.6638195514678955\n",
            "Epoch [92/500] Loss G: 1.7039318084716797 Loss D_H: 0.6922332048416138 Loss D_M: 0.690406322479248\n",
            "Epoch [93/500] Loss G: 1.7124335765838623 Loss D_H: 0.6927852630615234 Loss D_M: 0.6839591264724731\n",
            "Epoch [94/500] Loss G: 1.7376338243484497 Loss D_H: 0.6927062273025513 Loss D_M: 0.6848580241203308\n",
            "Epoch [95/500] Loss G: 1.6226637363433838 Loss D_H: 0.6927682757377625 Loss D_M: 0.6921406984329224\n",
            "Epoch [96/500] Loss G: 1.8231091499328613 Loss D_H: 0.6927437782287598 Loss D_M: 0.6797637939453125\n",
            "Epoch [97/500] Loss G: 1.729236364364624 Loss D_H: 0.6925818920135498 Loss D_M: 0.6862753629684448\n",
            "Epoch [98/500] Loss G: 1.673839807510376 Loss D_H: 0.692550539970398 Loss D_M: 0.6876314878463745\n",
            "Epoch [99/500] Loss G: 1.8586997985839844 Loss D_H: 0.692481279373169 Loss D_M: 0.6787623167037964\n",
            "Epoch [100/500] Loss G: 1.705869436264038 Loss D_H: 0.6928356885910034 Loss D_M: 0.6807641983032227\n",
            "Epoch [101/500] Loss G: 1.7067315578460693 Loss D_H: 0.6928498148918152 Loss D_M: 0.6866604089736938\n",
            "Epoch [102/500] Loss G: 1.6397770643234253 Loss D_H: 0.6933413743972778 Loss D_M: 0.6854392290115356\n",
            "Epoch [103/500] Loss G: 1.733933448791504 Loss D_H: 0.6925057172775269 Loss D_M: 0.6816377639770508\n",
            "Epoch [104/500] Loss G: 1.6454296112060547 Loss D_H: 0.6928300261497498 Loss D_M: 0.6831467151641846\n",
            "Epoch [105/500] Loss G: 1.682906150817871 Loss D_H: 0.6922788619995117 Loss D_M: 0.6853823661804199\n",
            "Epoch [106/500] Loss G: 1.7533724308013916 Loss D_H: 0.6925894618034363 Loss D_M: 0.6777082085609436\n",
            "Epoch [107/500] Loss G: 1.7506654262542725 Loss D_H: 0.6928538680076599 Loss D_M: 0.6721736192703247\n",
            "Epoch [108/500] Loss G: 1.7003169059753418 Loss D_H: 0.6924888491630554 Loss D_M: 0.6809597611427307\n",
            "Epoch [109/500] Loss G: 1.6486790180206299 Loss D_H: 0.6924537420272827 Loss D_M: 0.6842284202575684\n",
            "Epoch [110/500] Loss G: 1.7382904291152954 Loss D_H: 0.6919316053390503 Loss D_M: 0.686284065246582\n",
            "Epoch [111/500] Loss G: 1.775567650794983 Loss D_H: 0.6925538778305054 Loss D_M: 0.6754429936408997\n",
            "Epoch [112/500] Loss G: 1.7005980014801025 Loss D_H: 0.6924450397491455 Loss D_M: 0.6869125366210938\n",
            "Epoch [113/500] Loss G: 1.6471202373504639 Loss D_H: 0.6923251152038574 Loss D_M: 0.679524302482605\n",
            "Epoch [114/500] Loss G: 1.6540799140930176 Loss D_H: 0.6921781301498413 Loss D_M: 0.6886043548583984\n",
            "Epoch [115/500] Loss G: 1.6177926063537598 Loss D_H: 0.6929811239242554 Loss D_M: 0.6737807989120483\n",
            "Epoch [116/500] Loss G: 1.6481255292892456 Loss D_H: 0.6926854252815247 Loss D_M: 0.6778331995010376\n",
            "Epoch [117/500] Loss G: 1.6551427841186523 Loss D_H: 0.6925173401832581 Loss D_M: 0.6744741201400757\n",
            "Epoch [118/500] Loss G: 1.6453872919082642 Loss D_H: 0.6929644346237183 Loss D_M: 0.6818752884864807\n",
            "Epoch [119/500] Loss G: 1.6499077081680298 Loss D_H: 0.6924306750297546 Loss D_M: 0.6847206354141235\n",
            "Epoch [120/500] Loss G: 1.561227798461914 Loss D_H: 0.6931076645851135 Loss D_M: 0.6794369220733643\n",
            "Epoch [121/500] Loss G: 1.8014307022094727 Loss D_H: 0.6928024291992188 Loss D_M: 0.6509933471679688\n",
            "Epoch [122/500] Loss G: 1.7583281993865967 Loss D_H: 0.6933179497718811 Loss D_M: 0.6492918729782104\n",
            "Epoch [123/500] Loss G: 1.6742573976516724 Loss D_H: 0.6927869319915771 Loss D_M: 0.6864928007125854\n",
            "Epoch [124/500] Loss G: 1.6213445663452148 Loss D_H: 0.6926257610321045 Loss D_M: 0.6679984331130981\n",
            "Epoch [125/500] Loss G: 1.8041325807571411 Loss D_H: 0.6931682825088501 Loss D_M: 0.6487604379653931\n",
            "Epoch [126/500] Loss G: 1.5596340894699097 Loss D_H: 0.6933645009994507 Loss D_M: 0.6944047212600708\n",
            "Epoch [127/500] Loss G: 1.6263319253921509 Loss D_H: 0.6922776699066162 Loss D_M: 0.6882902383804321\n",
            "Epoch [128/500] Loss G: 1.5784920454025269 Loss D_H: 0.6924114227294922 Loss D_M: 0.6808137893676758\n",
            "Epoch [129/500] Loss G: 1.6452009677886963 Loss D_H: 0.6922732591629028 Loss D_M: 0.6668105125427246\n",
            "Epoch [130/500] Loss G: 1.630583643913269 Loss D_H: 0.6906579732894897 Loss D_M: 0.6644304990768433\n",
            "Epoch [131/500] Loss G: 1.6129577159881592 Loss D_H: 0.6929996013641357 Loss D_M: 0.7319661378860474\n",
            "Epoch [132/500] Loss G: 1.561245322227478 Loss D_H: 0.6927924156188965 Loss D_M: 0.7223268747329712\n",
            "Epoch [133/500] Loss G: 1.6618908643722534 Loss D_H: 0.6929658651351929 Loss D_M: 0.7277555465698242\n",
            "Epoch [134/500] Loss G: 1.6808338165283203 Loss D_H: 0.6905632019042969 Loss D_M: 0.7209042310714722\n",
            "Epoch [135/500] Loss G: 1.6496241092681885 Loss D_H: 0.6893963813781738 Loss D_M: 0.7000564336776733\n",
            "Epoch [136/500] Loss G: 1.7088767290115356 Loss D_H: 0.6833511590957642 Loss D_M: 0.6278063058853149\n",
            "Epoch [137/500] Loss G: 1.7396050691604614 Loss D_H: 0.6814467310905457 Loss D_M: 0.6026168465614319\n",
            "Epoch [138/500] Loss G: 1.8085300922393799 Loss D_H: 0.6780227422714233 Loss D_M: 0.5940526723861694\n",
            "Epoch [139/500] Loss G: 1.758322834968567 Loss D_H: 0.6828151345252991 Loss D_M: 0.6203728318214417\n",
            "Epoch [140/500] Loss G: 1.6327815055847168 Loss D_H: 0.6828186511993408 Loss D_M: 0.6321791410446167\n",
            "Epoch [141/500] Loss G: 1.7280027866363525 Loss D_H: 0.70600426197052 Loss D_M: 0.7747149467468262\n",
            "Epoch [142/500] Loss G: 1.5009139776229858 Loss D_H: 0.7038555145263672 Loss D_M: 0.7628074884414673\n",
            "Epoch [143/500] Loss G: 1.6731693744659424 Loss D_H: 0.7071660757064819 Loss D_M: 0.7438377141952515\n",
            "Epoch [144/500] Loss G: 1.683203935623169 Loss D_H: 0.706342339515686 Loss D_M: 0.766243577003479\n",
            "Epoch [145/500] Loss G: 1.5631022453308105 Loss D_H: 0.7002254724502563 Loss D_M: 0.7377059459686279\n",
            "Epoch [146/500] Loss G: 1.571273684501648 Loss D_H: 0.6977524161338806 Loss D_M: 0.7398080825805664\n",
            "Epoch [147/500] Loss G: 1.5885515213012695 Loss D_H: 0.6940485835075378 Loss D_M: 0.7238792181015015\n",
            "Epoch [148/500] Loss G: 1.610841155052185 Loss D_H: 0.6924411058425903 Loss D_M: 0.7212402820587158\n",
            "Epoch [149/500] Loss G: 1.6362667083740234 Loss D_H: 0.6895772218704224 Loss D_M: 0.7100155353546143\n",
            "Epoch [150/500] Loss G: 1.6352968215942383 Loss D_H: 0.6839312314987183 Loss D_M: 0.7062058448791504\n",
            "Epoch [151/500] Loss G: 1.64840829372406 Loss D_H: 0.6842318773269653 Loss D_M: 0.6950773000717163\n",
            "Epoch [152/500] Loss G: 1.5939583778381348 Loss D_H: 0.6754775047302246 Loss D_M: 0.6904182434082031\n",
            "Epoch [153/500] Loss G: 1.5994141101837158 Loss D_H: 0.6745848655700684 Loss D_M: 0.6833332777023315\n",
            "Epoch [154/500] Loss G: 1.6613218784332275 Loss D_H: 0.6685044765472412 Loss D_M: 0.6775075793266296\n",
            "Epoch [155/500] Loss G: 1.7409017086029053 Loss D_H: 0.6634811162948608 Loss D_M: 0.6713137626647949\n",
            "Epoch [156/500] Loss G: 1.642259120941162 Loss D_H: 0.6596630811691284 Loss D_M: 0.6698299646377563\n",
            "Epoch [157/500] Loss G: 1.6478852033615112 Loss D_H: 0.6564982533454895 Loss D_M: 0.6684277057647705\n",
            "Epoch [158/500] Loss G: 1.7109986543655396 Loss D_H: 0.6633908748626709 Loss D_M: 0.6738532781600952\n",
            "Epoch [159/500] Loss G: 1.6640141010284424 Loss D_H: 0.66796875 Loss D_M: 0.6769639253616333\n",
            "Epoch [160/500] Loss G: 1.6599634885787964 Loss D_H: 0.6895421147346497 Loss D_M: 0.6901078224182129\n",
            "Epoch [161/500] Loss G: 1.5512661933898926 Loss D_H: 0.6830973029136658 Loss D_M: 0.6872535347938538\n",
            "Epoch [162/500] Loss G: 1.5889123678207397 Loss D_H: 0.7089567184448242 Loss D_M: 0.7081378698348999\n",
            "Epoch [163/500] Loss G: 1.7061700820922852 Loss D_H: 0.6959291696548462 Loss D_M: 0.6902569532394409\n",
            "Epoch [164/500] Loss G: 1.5609838962554932 Loss D_H: 0.7084500789642334 Loss D_M: 0.7051107883453369\n",
            "Epoch [165/500] Loss G: 1.5420032739639282 Loss D_H: 0.7061222791671753 Loss D_M: 0.7036937475204468\n",
            "Epoch [166/500] Loss G: 1.6700228452682495 Loss D_H: 0.7109194993972778 Loss D_M: 0.7015880346298218\n",
            "Epoch [167/500] Loss G: 1.5662107467651367 Loss D_H: 0.6986844539642334 Loss D_M: 0.690514087677002\n",
            "Epoch [168/500] Loss G: 1.6531552076339722 Loss D_H: 0.7001051902770996 Loss D_M: 0.6872053146362305\n",
            "Epoch [169/500] Loss G: 1.5886647701263428 Loss D_H: 0.6951066255569458 Loss D_M: 0.6863401532173157\n",
            "Epoch [170/500] Loss G: 1.6611883640289307 Loss D_H: 0.6955910325050354 Loss D_M: 0.6865847110748291\n",
            "Epoch [171/500] Loss G: 1.672994613647461 Loss D_H: 0.6858630776405334 Loss D_M: 0.6632194519042969\n",
            "Epoch [172/500] Loss G: 1.7434751987457275 Loss D_H: 0.6866440176963806 Loss D_M: 0.6583107113838196\n",
            "Epoch [173/500] Loss G: 1.6134815216064453 Loss D_H: 0.6946909427642822 Loss D_M: 0.6871200799942017\n",
            "Epoch [174/500] Loss G: 1.6526505947113037 Loss D_H: 0.6910524368286133 Loss D_M: 0.6905086636543274\n",
            "Epoch [175/500] Loss G: 1.60879647731781 Loss D_H: 0.6920263767242432 Loss D_M: 0.689422607421875\n",
            "Epoch [176/500] Loss G: 1.6974046230316162 Loss D_H: 0.6962405443191528 Loss D_M: 0.6899034976959229\n",
            "Epoch [177/500] Loss G: 1.5779743194580078 Loss D_H: 0.6930323839187622 Loss D_M: 0.6899697780609131\n",
            "Epoch [178/500] Loss G: 1.5702086687088013 Loss D_H: 0.6916601657867432 Loss D_M: 0.6994779109954834\n",
            "Epoch [179/500] Loss G: 1.6387978792190552 Loss D_H: 0.6762739419937134 Loss D_M: 0.6744648218154907\n",
            "Epoch [180/500] Loss G: 1.6453007459640503 Loss D_H: 0.6919447183609009 Loss D_M: 0.6936931610107422\n",
            "Epoch [181/500] Loss G: 1.5664658546447754 Loss D_H: 0.6911715269088745 Loss D_M: 0.6948022842407227\n",
            "Epoch [182/500] Loss G: 1.5567729473114014 Loss D_H: 0.6893054842948914 Loss D_M: 0.6952102184295654\n",
            "Epoch [183/500] Loss G: 1.6332082748413086 Loss D_H: 0.6933892369270325 Loss D_M: 0.6950768232345581\n",
            "Epoch [184/500] Loss G: 1.5811105966567993 Loss D_H: 0.6919612884521484 Loss D_M: 0.6939276456832886\n",
            "Epoch [185/500] Loss G: 1.6068097352981567 Loss D_H: 0.6864501237869263 Loss D_M: 0.6865162253379822\n",
            "Epoch [186/500] Loss G: 1.6429916620254517 Loss D_H: 0.6896485090255737 Loss D_M: 0.6947295069694519\n",
            "Epoch [187/500] Loss G: 1.640166997909546 Loss D_H: 0.697221577167511 Loss D_M: 0.690321683883667\n",
            "Epoch [188/500] Loss G: 1.5873750448226929 Loss D_H: 0.6908082962036133 Loss D_M: 0.6912506818771362\n",
            "Epoch [189/500] Loss G: 1.5259708166122437 Loss D_H: 0.700620174407959 Loss D_M: 0.6903985738754272\n",
            "Epoch [190/500] Loss G: 1.6318459510803223 Loss D_H: 0.699516236782074 Loss D_M: 0.6926009654998779\n",
            "Epoch [191/500] Loss G: 1.5543835163116455 Loss D_H: 0.6982773542404175 Loss D_M: 0.6786953806877136\n",
            "Epoch [192/500] Loss G: 1.5476683378219604 Loss D_H: 0.6894489526748657 Loss D_M: 0.6796212196350098\n",
            "Epoch [193/500] Loss G: 1.5736156702041626 Loss D_H: 0.6952650547027588 Loss D_M: 0.6846389770507812\n",
            "Epoch [194/500] Loss G: 1.5188264846801758 Loss D_H: 0.6939489841461182 Loss D_M: 0.6936314702033997\n",
            "Epoch [195/500] Loss G: 1.5547916889190674 Loss D_H: 0.694783091545105 Loss D_M: 0.6927533149719238\n",
            "Epoch [196/500] Loss G: 1.70622718334198 Loss D_H: 0.67490154504776 Loss D_M: 0.6591929197311401\n",
            "Epoch [197/500] Loss G: 1.6506820917129517 Loss D_H: 0.6895668506622314 Loss D_M: 0.6869348883628845\n",
            "Epoch [198/500] Loss G: 1.610458493232727 Loss D_H: 0.6967977285385132 Loss D_M: 0.6945317983627319\n",
            "Epoch [199/500] Loss G: 1.611708641052246 Loss D_H: 0.696532130241394 Loss D_M: 0.6925347447395325\n",
            "Epoch [200/500] Loss G: 1.550758719444275 Loss D_H: 0.6931934952735901 Loss D_M: 0.6809326410293579\n",
            "Epoch [201/500] Loss G: 1.7419527769088745 Loss D_H: 0.6789253950119019 Loss D_M: 0.6525314450263977\n",
            "Epoch [202/500] Loss G: 1.6007106304168701 Loss D_H: 0.6916452646255493 Loss D_M: 0.68436199426651\n",
            "Epoch [203/500] Loss G: 1.5994576215744019 Loss D_H: 0.6870596408843994 Loss D_M: 0.6915191411972046\n",
            "Epoch [204/500] Loss G: 1.600083589553833 Loss D_H: 0.6950784921646118 Loss D_M: 0.6937410831451416\n",
            "Epoch [205/500] Loss G: 1.6372233629226685 Loss D_H: 0.6887938976287842 Loss D_M: 0.692216694355011\n",
            "Epoch [206/500] Loss G: 1.5478709936141968 Loss D_H: 0.6972411274909973 Loss D_M: 0.690770149230957\n",
            "Epoch [207/500] Loss G: 1.6413090229034424 Loss D_H: 0.6940696835517883 Loss D_M: 0.6804800629615784\n",
            "Epoch [208/500] Loss G: 1.6013739109039307 Loss D_H: 0.6948074102401733 Loss D_M: 0.6869598627090454\n",
            "Epoch [209/500] Loss G: 1.6207022666931152 Loss D_H: 0.692388653755188 Loss D_M: 0.6902363300323486\n",
            "Epoch [210/500] Loss G: 1.609039306640625 Loss D_H: 0.694738507270813 Loss D_M: 0.6835688352584839\n",
            "Epoch [211/500] Loss G: 1.6656267642974854 Loss D_H: 0.6915431022644043 Loss D_M: 0.6841384172439575\n",
            "Epoch [212/500] Loss G: 1.5816644430160522 Loss D_H: 0.6940809488296509 Loss D_M: 0.7021607756614685\n",
            "Epoch [213/500] Loss G: 1.6142239570617676 Loss D_H: 0.6935487985610962 Loss D_M: 0.6920242309570312\n",
            "Epoch [214/500] Loss G: 1.5938876867294312 Loss D_H: 0.6901816129684448 Loss D_M: 0.6817463636398315\n",
            "Epoch [215/500] Loss G: 1.5572469234466553 Loss D_H: 0.6955435276031494 Loss D_M: 0.6923196315765381\n",
            "Epoch [216/500] Loss G: 1.5934571027755737 Loss D_H: 0.6935024261474609 Loss D_M: 0.6875272989273071\n",
            "Epoch [217/500] Loss G: 1.6686517000198364 Loss D_H: 0.6733070611953735 Loss D_M: 0.6435381174087524\n",
            "Epoch [218/500] Loss G: 1.561450481414795 Loss D_H: 0.6935024261474609 Loss D_M: 0.6899181604385376\n",
            "Epoch [219/500] Loss G: 1.5613389015197754 Loss D_H: 0.6915386915206909 Loss D_M: 0.6860543489456177\n",
            "Epoch [220/500] Loss G: 1.6197917461395264 Loss D_H: 0.6727343797683716 Loss D_M: 0.6559461355209351\n",
            "Epoch [221/500] Loss G: 1.613097906112671 Loss D_H: 0.6919498443603516 Loss D_M: 0.6908025741577148\n",
            "Epoch [222/500] Loss G: 1.6269670724868774 Loss D_H: 0.6885181665420532 Loss D_M: 0.675589919090271\n",
            "Epoch [223/500] Loss G: 1.6445237398147583 Loss D_H: 0.6753476858139038 Loss D_M: 0.6540238857269287\n",
            "Epoch [224/500] Loss G: 1.5406785011291504 Loss D_H: 0.6894149780273438 Loss D_M: 0.6836081743240356\n",
            "Epoch [225/500] Loss G: 1.6224949359893799 Loss D_H: 0.6915391683578491 Loss D_M: 0.6857314109802246\n",
            "Epoch [226/500] Loss G: 1.559799313545227 Loss D_H: 0.6856590509414673 Loss D_M: 0.6812894940376282\n",
            "Epoch [227/500] Loss G: 1.606929063796997 Loss D_H: 0.6927669644355774 Loss D_M: 0.6828112602233887\n",
            "Epoch [228/500] Loss G: 1.5600148439407349 Loss D_H: 0.6874160170555115 Loss D_M: 0.6838456392288208\n",
            "Epoch [229/500] Loss G: 1.5892935991287231 Loss D_H: 0.6853737235069275 Loss D_M: 0.6800094842910767\n",
            "Epoch [230/500] Loss G: 1.5357465744018555 Loss D_H: 0.6952401399612427 Loss D_M: 0.6907953023910522\n",
            "Epoch [231/500] Loss G: 1.6289989948272705 Loss D_H: 0.6875700950622559 Loss D_M: 0.6827013492584229\n",
            "Epoch [232/500] Loss G: 1.6045701503753662 Loss D_H: 0.6900782585144043 Loss D_M: 0.677794337272644\n",
            "Epoch [233/500] Loss G: 1.5316541194915771 Loss D_H: 0.6891968250274658 Loss D_M: 0.68230140209198\n",
            "Epoch [234/500] Loss G: 1.5081133842468262 Loss D_H: 0.6910394430160522 Loss D_M: 0.6898759603500366\n",
            "Epoch [235/500] Loss G: 1.635861873626709 Loss D_H: 0.6907716989517212 Loss D_M: 0.6747166514396667\n",
            "Epoch [236/500] Loss G: 1.5630943775177002 Loss D_H: 0.6940712928771973 Loss D_M: 0.6920734643936157\n",
            "Epoch [237/500] Loss G: 1.574841856956482 Loss D_H: 0.6841082572937012 Loss D_M: 0.6789827942848206\n",
            "Epoch [238/500] Loss G: 1.5355134010314941 Loss D_H: 0.688435435295105 Loss D_M: 0.685870885848999\n",
            "Epoch [239/500] Loss G: 1.6075116395950317 Loss D_H: 0.6948457956314087 Loss D_M: 0.689576268196106\n",
            "Epoch [240/500] Loss G: 1.5595132112503052 Loss D_H: 0.6929833889007568 Loss D_M: 0.6872113943099976\n",
            "Epoch [241/500] Loss G: 1.6604067087173462 Loss D_H: 0.6940299868583679 Loss D_M: 0.6894911527633667\n",
            "Epoch [242/500] Loss G: 1.6263176202774048 Loss D_H: 0.6903935074806213 Loss D_M: 0.6809679269790649\n",
            "Epoch [243/500] Loss G: 1.6136603355407715 Loss D_H: 0.6914421319961548 Loss D_M: 0.683774471282959\n",
            "Epoch [244/500] Loss G: 1.6354843378067017 Loss D_H: 0.6918010711669922 Loss D_M: 0.6845700740814209\n",
            "Epoch [245/500] Loss G: 1.615638017654419 Loss D_H: 0.6912658214569092 Loss D_M: 0.6816533803939819\n",
            "Epoch [246/500] Loss G: 1.5558631420135498 Loss D_H: 0.6880908608436584 Loss D_M: 0.6825989484786987\n",
            "Epoch [247/500] Loss G: 1.595491886138916 Loss D_H: 0.6702197790145874 Loss D_M: 0.6706236600875854\n",
            "Epoch [248/500] Loss G: 1.638497233390808 Loss D_H: 0.684606671333313 Loss D_M: 0.7059005498886108\n",
            "Epoch [249/500] Loss G: 1.6304283142089844 Loss D_H: 0.6931931972503662 Loss D_M: 0.6894744634628296\n",
            "Epoch [250/500] Loss G: 1.6121124029159546 Loss D_H: 0.6898617148399353 Loss D_M: 0.6800101399421692\n",
            "Epoch [251/500] Loss G: 1.5693391561508179 Loss D_H: 0.688606858253479 Loss D_M: 0.6848143935203552\n",
            "Epoch [252/500] Loss G: 1.6107100248336792 Loss D_H: 0.6950308680534363 Loss D_M: 0.7255167365074158\n",
            "Epoch [253/500] Loss G: 1.6080948114395142 Loss D_H: 0.6922632455825806 Loss D_M: 0.7258721590042114\n",
            "Epoch [254/500] Loss G: 1.645287036895752 Loss D_H: 0.6856898069381714 Loss D_M: 0.6687592267990112\n",
            "Epoch [255/500] Loss G: 1.7096418142318726 Loss D_H: 0.6701717376708984 Loss D_M: 0.6391211748123169\n",
            "Epoch [256/500] Loss G: 1.6287082433700562 Loss D_H: 0.6743468642234802 Loss D_M: 0.6474295854568481\n",
            "Epoch [257/500] Loss G: 1.5923128128051758 Loss D_H: 0.6778234839439392 Loss D_M: 0.6516228914260864\n",
            "Epoch [258/500] Loss G: 1.5426796674728394 Loss D_H: 0.6849637031555176 Loss D_M: 0.6677929759025574\n",
            "Epoch [259/500] Loss G: 1.5357733964920044 Loss D_H: 0.6971220374107361 Loss D_M: 0.7316393852233887\n",
            "Epoch [260/500] Loss G: 1.6743645668029785 Loss D_H: 0.6705101728439331 Loss D_M: 0.682341456413269\n",
            "Epoch [261/500] Loss G: 1.5197252035140991 Loss D_H: 0.6909211277961731 Loss D_M: 0.7052937746047974\n",
            "Epoch [262/500] Loss G: 1.5643432140350342 Loss D_H: 0.6869536638259888 Loss D_M: 0.7148929238319397\n",
            "Epoch [263/500] Loss G: 1.5460402965545654 Loss D_H: 0.6791032552719116 Loss D_M: 0.7194451093673706\n",
            "Epoch [264/500] Loss G: 1.5747233629226685 Loss D_H: 0.6860705614089966 Loss D_M: 0.7033191919326782\n",
            "Epoch [265/500] Loss G: 1.5578603744506836 Loss D_H: 0.6615338325500488 Loss D_M: 0.7196098566055298\n",
            "Epoch [266/500] Loss G: 1.6819435358047485 Loss D_H: 0.6458866000175476 Loss D_M: 0.6764199733734131\n",
            "Epoch [267/500] Loss G: 1.6169573068618774 Loss D_H: 0.6620604991912842 Loss D_M: 0.7128913402557373\n",
            "Epoch [268/500] Loss G: 1.5427348613739014 Loss D_H: 0.6871329545974731 Loss D_M: 0.6987927556037903\n",
            "Epoch [269/500] Loss G: 1.6099070310592651 Loss D_H: 0.6798559427261353 Loss D_M: 0.6994402408599854\n",
            "Epoch [270/500] Loss G: 1.5799480676651 Loss D_H: 0.7032229900360107 Loss D_M: 0.6899669766426086\n",
            "Epoch [271/500] Loss G: 1.601423978805542 Loss D_H: 0.6899992227554321 Loss D_M: 0.6936073899269104\n",
            "Epoch [272/500] Loss G: 1.589168667793274 Loss D_H: 0.6768862009048462 Loss D_M: 0.7017899751663208\n",
            "Epoch [273/500] Loss G: 1.5540000200271606 Loss D_H: 0.7004601955413818 Loss D_M: 0.6817026138305664\n",
            "Epoch [274/500] Loss G: 1.642957091331482 Loss D_H: 0.6900919675827026 Loss D_M: 0.6881494522094727\n",
            "Epoch [275/500] Loss G: 1.638176441192627 Loss D_H: 0.6700387597084045 Loss D_M: 0.645270824432373\n",
            "Epoch [276/500] Loss G: 1.562638759613037 Loss D_H: 0.6943347454071045 Loss D_M: 0.6892397403717041\n",
            "Epoch [277/500] Loss G: 1.5847349166870117 Loss D_H: 0.691440224647522 Loss D_M: 0.6891570091247559\n",
            "Epoch [278/500] Loss G: 1.6475212574005127 Loss D_H: 0.6858134269714355 Loss D_M: 0.6906511783599854\n",
            "Epoch [279/500] Loss G: 1.5448503494262695 Loss D_H: 0.6927451491355896 Loss D_M: 0.6878646612167358\n",
            "Epoch [280/500] Loss G: 1.5847834348678589 Loss D_H: 0.6894453167915344 Loss D_M: 0.6952635049819946\n",
            "Epoch [281/500] Loss G: 1.643105149269104 Loss D_H: 0.694226086139679 Loss D_M: 0.6836860179901123\n",
            "Epoch [282/500] Loss G: 1.636896014213562 Loss D_H: 0.6887068748474121 Loss D_M: 0.684023380279541\n",
            "Epoch [283/500] Loss G: 1.6315785646438599 Loss D_H: 0.6972687244415283 Loss D_M: 0.6826527118682861\n",
            "Epoch [284/500] Loss G: 1.610289454460144 Loss D_H: 0.6898047924041748 Loss D_M: 0.6934382915496826\n",
            "Epoch [285/500] Loss G: 1.6010198593139648 Loss D_H: 0.6910456418991089 Loss D_M: 0.692054271697998\n",
            "Epoch [286/500] Loss G: 1.5435553789138794 Loss D_H: 0.6833494901657104 Loss D_M: 0.6796165704727173\n",
            "Epoch [287/500] Loss G: 1.583455204963684 Loss D_H: 0.6932389140129089 Loss D_M: 0.6850355863571167\n",
            "Epoch [288/500] Loss G: 1.5746835470199585 Loss D_H: 0.6666685342788696 Loss D_M: 0.6595914363861084\n",
            "Epoch [289/500] Loss G: 1.610286831855774 Loss D_H: 0.6958584189414978 Loss D_M: 0.6925897598266602\n",
            "Epoch [290/500] Loss G: 1.6071513891220093 Loss D_H: 0.6855577230453491 Loss D_M: 0.6915758848190308\n",
            "Epoch [291/500] Loss G: 1.551556944847107 Loss D_H: 0.6886786222457886 Loss D_M: 0.6908453702926636\n",
            "Epoch [292/500] Loss G: 1.5519499778747559 Loss D_H: 0.691098690032959 Loss D_M: 0.6931157112121582\n",
            "Epoch [293/500] Loss G: 1.6275947093963623 Loss D_H: 0.6953672170639038 Loss D_M: 0.693543016910553\n",
            "Epoch [294/500] Loss G: 1.5936986207962036 Loss D_H: 0.6983594298362732 Loss D_M: 0.6975218057632446\n",
            "Epoch [295/500] Loss G: 1.608938217163086 Loss D_H: 0.6874422430992126 Loss D_M: 0.6805910468101501\n",
            "Epoch [296/500] Loss G: 1.611825704574585 Loss D_H: 0.6936285495758057 Loss D_M: 0.6857017278671265\n",
            "Epoch [297/500] Loss G: 1.5883848667144775 Loss D_H: 0.6874555349349976 Loss D_M: 0.6756898164749146\n",
            "Epoch [298/500] Loss G: 1.585788369178772 Loss D_H: 0.6942819952964783 Loss D_M: 0.6963144540786743\n",
            "Epoch [299/500] Loss G: 1.6001232862472534 Loss D_H: 0.6954202651977539 Loss D_M: 0.7082792520523071\n",
            "Epoch [300/500] Loss G: 1.5451340675354004 Loss D_H: 0.6892672777175903 Loss D_M: 0.7116305828094482\n",
            "Epoch [301/500] Loss G: 1.592686653137207 Loss D_H: 0.6852707862854004 Loss D_M: 0.6982435584068298\n",
            "Epoch [302/500] Loss G: 1.5551233291625977 Loss D_H: 0.6839897632598877 Loss D_M: 0.6920640468597412\n",
            "Epoch [303/500] Loss G: 1.522634744644165 Loss D_H: 0.7008783221244812 Loss D_M: 0.6939260959625244\n",
            "Epoch [304/500] Loss G: 1.674655556678772 Loss D_H: 0.6653831005096436 Loss D_M: 0.6425297260284424\n",
            "Epoch [305/500] Loss G: 1.6149176359176636 Loss D_H: 0.6916106343269348 Loss D_M: 0.6901726722717285\n",
            "Epoch [306/500] Loss G: 1.5264678001403809 Loss D_H: 0.7048519253730774 Loss D_M: 0.732871949672699\n",
            "Epoch [307/500] Loss G: 1.5967481136322021 Loss D_H: 0.685634195804596 Loss D_M: 0.6890264749526978\n",
            "Epoch [308/500] Loss G: 1.6496384143829346 Loss D_H: 0.6812069416046143 Loss D_M: 0.6697986721992493\n",
            "Epoch [309/500] Loss G: 1.5508480072021484 Loss D_H: 0.6964722275733948 Loss D_M: 0.691463828086853\n",
            "Epoch [310/500] Loss G: 1.602447271347046 Loss D_H: 0.6868945360183716 Loss D_M: 0.6697069406509399\n",
            "Epoch [311/500] Loss G: 1.6098437309265137 Loss D_H: 0.68936687707901 Loss D_M: 0.6994647979736328\n",
            "Epoch [312/500] Loss G: 1.5898526906967163 Loss D_H: 0.6944780349731445 Loss D_M: 0.7065212726593018\n",
            "Epoch [313/500] Loss G: 1.5879110097885132 Loss D_H: 0.6690821647644043 Loss D_M: 0.7194430232048035\n",
            "Epoch [314/500] Loss G: 1.5282121896743774 Loss D_H: 0.6933748722076416 Loss D_M: 0.6947653293609619\n",
            "Epoch [315/500] Loss G: 1.518538475036621 Loss D_H: 0.6801172494888306 Loss D_M: 0.7084460258483887\n",
            "Epoch [316/500] Loss G: 1.601090908050537 Loss D_H: 0.6764934062957764 Loss D_M: 0.7058448791503906\n",
            "Epoch [317/500] Loss G: 1.475677490234375 Loss D_H: 0.6880019903182983 Loss D_M: 0.698798656463623\n",
            "Epoch [318/500] Loss G: 1.5397026538848877 Loss D_H: 0.6955193281173706 Loss D_M: 0.6906236410140991\n",
            "Epoch [319/500] Loss G: 1.553339958190918 Loss D_H: 0.6936348676681519 Loss D_M: 0.6892833709716797\n",
            "Epoch [320/500] Loss G: 1.5869920253753662 Loss D_H: 0.6936827898025513 Loss D_M: 0.689154863357544\n",
            "Epoch [321/500] Loss G: 1.5633906126022339 Loss D_H: 0.688566267490387 Loss D_M: 0.690734326839447\n",
            "Epoch [322/500] Loss G: 1.576481580734253 Loss D_H: 0.6872569918632507 Loss D_M: 0.6916706562042236\n",
            "Epoch [323/500] Loss G: 1.6112170219421387 Loss D_H: 0.6814347505569458 Loss D_M: 0.6797105073928833\n",
            "Epoch [324/500] Loss G: 1.6228684186935425 Loss D_H: 0.6765624284744263 Loss D_M: 0.6458818912506104\n",
            "Epoch [325/500] Loss G: 1.59138023853302 Loss D_H: 0.7059899568557739 Loss D_M: 0.6896870136260986\n",
            "Epoch [326/500] Loss G: 1.5836824178695679 Loss D_H: 0.6940159797668457 Loss D_M: 0.6908444166183472\n",
            "Epoch [327/500] Loss G: 1.5941580533981323 Loss D_H: 0.6833155155181885 Loss D_M: 0.6724240779876709\n",
            "Epoch [328/500] Loss G: 1.636533260345459 Loss D_H: 0.6974799036979675 Loss D_M: 0.6940787434577942\n",
            "Epoch [329/500] Loss G: 1.5490628480911255 Loss D_H: 0.6953831911087036 Loss D_M: 0.689572274684906\n",
            "Epoch [330/500] Loss G: 1.6270169019699097 Loss D_H: 0.6836483478546143 Loss D_M: 0.6815252900123596\n",
            "Epoch [331/500] Loss G: 1.6025729179382324 Loss D_H: 0.6835540533065796 Loss D_M: 0.7015500068664551\n",
            "Epoch [332/500] Loss G: 1.6224994659423828 Loss D_H: 0.6866896152496338 Loss D_M: 0.6959391832351685\n",
            "Epoch [333/500] Loss G: 1.5560581684112549 Loss D_H: 0.6818727850914001 Loss D_M: 0.6934430599212646\n",
            "Epoch [334/500] Loss G: 1.6702165603637695 Loss D_H: 0.687092661857605 Loss D_M: 0.6930164694786072\n",
            "Epoch [335/500] Loss G: 1.5506057739257812 Loss D_H: 0.6893854141235352 Loss D_M: 0.6922396421432495\n",
            "Epoch [336/500] Loss G: 1.6112396717071533 Loss D_H: 0.695644736289978 Loss D_M: 0.6872215867042542\n",
            "Epoch [337/500] Loss G: 1.5558264255523682 Loss D_H: 0.6991078853607178 Loss D_M: 0.695288360118866\n",
            "Epoch [338/500] Loss G: 1.5613218545913696 Loss D_H: 0.6885987520217896 Loss D_M: 0.6929850578308105\n",
            "Epoch [339/500] Loss G: 1.5392673015594482 Loss D_H: 0.6904592514038086 Loss D_M: 0.6938780546188354\n",
            "Epoch [340/500] Loss G: 1.5654277801513672 Loss D_H: 0.6844565272331238 Loss D_M: 0.6959266662597656\n",
            "Epoch [341/500] Loss G: 1.5706400871276855 Loss D_H: 0.6996670961380005 Loss D_M: 0.693669319152832\n",
            "Epoch [342/500] Loss G: 1.606536865234375 Loss D_H: 0.6874849796295166 Loss D_M: 0.6850751638412476\n",
            "Epoch [343/500] Loss G: 1.5325422286987305 Loss D_H: 0.6926367282867432 Loss D_M: 0.693044900894165\n",
            "Epoch [344/500] Loss G: 1.6307810544967651 Loss D_H: 0.6960029006004333 Loss D_M: 0.6789600253105164\n",
            "Epoch [345/500] Loss G: 1.5119508504867554 Loss D_H: 0.7006509304046631 Loss D_M: 0.6934472918510437\n",
            "Epoch [346/500] Loss G: 1.5773993730545044 Loss D_H: 0.6985181570053101 Loss D_M: 0.691339373588562\n",
            "Epoch [347/500] Loss G: 1.5675288438796997 Loss D_H: 0.7030336856842041 Loss D_M: 0.685600996017456\n",
            "Epoch [348/500] Loss G: 1.5365815162658691 Loss D_H: 0.693304181098938 Loss D_M: 0.6982160806655884\n",
            "Epoch [349/500] Loss G: 1.5226383209228516 Loss D_H: 0.6936070322990417 Loss D_M: 0.690284013748169\n",
            "Epoch [350/500] Loss G: 1.6052567958831787 Loss D_H: 0.6877102851867676 Loss D_M: 0.687661349773407\n",
            "Epoch [351/500] Loss G: 1.6685967445373535 Loss D_H: 0.6660894155502319 Loss D_M: 0.645229697227478\n",
            "Epoch [352/500] Loss G: 1.534883737564087 Loss D_H: 0.7030011415481567 Loss D_M: 0.6920557022094727\n",
            "Epoch [353/500] Loss G: 1.6224589347839355 Loss D_H: 0.6981096267700195 Loss D_M: 0.6950337886810303\n",
            "Epoch [354/500] Loss G: 1.6706597805023193 Loss D_H: 0.6533573865890503 Loss D_M: 0.6541851162910461\n",
            "Epoch [355/500] Loss G: 1.6103544235229492 Loss D_H: 0.6923554539680481 Loss D_M: 0.6849933862686157\n",
            "Epoch [356/500] Loss G: 1.5379080772399902 Loss D_H: 0.6946519017219543 Loss D_M: 0.6926182508468628\n",
            "Epoch [357/500] Loss G: 1.5895780324935913 Loss D_H: 0.7043137550354004 Loss D_M: 0.6864138245582581\n",
            "Epoch [358/500] Loss G: 1.5059326887130737 Loss D_H: 0.6947513818740845 Loss D_M: 0.6917877793312073\n",
            "Epoch [359/500] Loss G: 1.6222107410430908 Loss D_H: 0.6866047382354736 Loss D_M: 0.7013850212097168\n",
            "Epoch [360/500] Loss G: 1.5119675397872925 Loss D_H: 0.6963148713111877 Loss D_M: 0.6833134293556213\n",
            "Epoch [361/500] Loss G: 1.5740110874176025 Loss D_H: 0.7004943490028381 Loss D_M: 0.6865665912628174\n",
            "Epoch [362/500] Loss G: 1.5488611459732056 Loss D_H: 0.7026156187057495 Loss D_M: 0.694263756275177\n",
            "Epoch [363/500] Loss G: 1.545396327972412 Loss D_H: 0.7000705003738403 Loss D_M: 0.6927914023399353\n",
            "Epoch [364/500] Loss G: 1.5751049518585205 Loss D_H: 0.6982825994491577 Loss D_M: 0.6908378601074219\n",
            "Epoch [365/500] Loss G: 1.5664052963256836 Loss D_H: 0.6959350109100342 Loss D_M: 0.6808664798736572\n",
            "Epoch [366/500] Loss G: 1.5496540069580078 Loss D_H: 0.6878516674041748 Loss D_M: 0.6812819242477417\n",
            "Epoch [367/500] Loss G: 1.541318655014038 Loss D_H: 0.6889314651489258 Loss D_M: 0.6885147094726562\n",
            "Epoch [368/500] Loss G: 1.6401934623718262 Loss D_H: 0.6712062954902649 Loss D_M: 0.6452635526657104\n",
            "Epoch [369/500] Loss G: 1.5665106773376465 Loss D_H: 0.6912996768951416 Loss D_M: 0.6990927457809448\n",
            "Epoch [370/500] Loss G: 1.5809602737426758 Loss D_H: 0.6928519010543823 Loss D_M: 0.6871598958969116\n",
            "Epoch [371/500] Loss G: 1.616302251815796 Loss D_H: 0.6818820238113403 Loss D_M: 0.6824455261230469\n",
            "Epoch [372/500] Loss G: 1.6321767568588257 Loss D_H: 0.6676027178764343 Loss D_M: 0.6513928174972534\n",
            "Epoch [373/500] Loss G: 1.5991570949554443 Loss D_H: 0.6742596626281738 Loss D_M: 0.6425199508666992\n",
            "Epoch [374/500] Loss G: 1.6486196517944336 Loss D_H: 0.6901502013206482 Loss D_M: 0.6915860176086426\n",
            "Epoch [375/500] Loss G: 1.6088842153549194 Loss D_H: 0.6937936544418335 Loss D_M: 0.6842033863067627\n",
            "Epoch [376/500] Loss G: 1.5958750247955322 Loss D_H: 0.6754422783851624 Loss D_M: 0.6405351161956787\n",
            "Epoch [377/500] Loss G: 1.6139822006225586 Loss D_H: 0.6925752758979797 Loss D_M: 0.6785507202148438\n",
            "Epoch [378/500] Loss G: 1.5739203691482544 Loss D_H: 0.6945786476135254 Loss D_M: 0.6851101517677307\n",
            "Epoch [379/500] Loss G: 1.5760209560394287 Loss D_H: 0.7035146951675415 Loss D_M: 0.6778296232223511\n",
            "Epoch [380/500] Loss G: 1.617655873298645 Loss D_H: 0.6918312311172485 Loss D_M: 0.6939725875854492\n",
            "Epoch [381/500] Loss G: 1.6524131298065186 Loss D_H: 0.6978663206100464 Loss D_M: 0.6723936200141907\n",
            "Epoch [382/500] Loss G: 1.5147637128829956 Loss D_H: 0.6957714557647705 Loss D_M: 0.6954420208930969\n",
            "Epoch [383/500] Loss G: 1.5750938653945923 Loss D_H: 0.697796642780304 Loss D_M: 0.688932478427887\n",
            "Epoch [384/500] Loss G: 1.5704197883605957 Loss D_H: 0.7025676965713501 Loss D_M: 0.6882966756820679\n",
            "Epoch [385/500] Loss G: 1.565476417541504 Loss D_H: 0.6935383677482605 Loss D_M: 0.6874760389328003\n",
            "Epoch [386/500] Loss G: 1.5715550184249878 Loss D_H: 0.7031873464584351 Loss D_M: 0.6814877986907959\n",
            "Epoch [387/500] Loss G: 1.5369793176651 Loss D_H: 0.7141196131706238 Loss D_M: 0.693081796169281\n",
            "Epoch [388/500] Loss G: 1.6431642770767212 Loss D_H: 0.6700387001037598 Loss D_M: 0.6427493691444397\n",
            "Epoch [389/500] Loss G: 1.5928938388824463 Loss D_H: 0.6956390142440796 Loss D_M: 0.6988058090209961\n",
            "Epoch [390/500] Loss G: 1.5447280406951904 Loss D_H: 0.6932571530342102 Loss D_M: 0.6928492784500122\n",
            "Epoch [391/500] Loss G: 1.5597347021102905 Loss D_H: 0.6913047432899475 Loss D_M: 0.6944971680641174\n",
            "Epoch [392/500] Loss G: 1.6510415077209473 Loss D_H: 0.694283127784729 Loss D_M: 0.686607837677002\n",
            "Epoch [393/500] Loss G: 1.5471279621124268 Loss D_H: 0.7043733596801758 Loss D_M: 0.6839176416397095\n",
            "Epoch [394/500] Loss G: 1.621889591217041 Loss D_H: 0.6639645099639893 Loss D_M: 0.6458783149719238\n",
            "Epoch [395/500] Loss G: 1.6467713117599487 Loss D_H: 0.6666065454483032 Loss D_M: 0.6397408843040466\n",
            "Epoch [396/500] Loss G: 1.6103994846343994 Loss D_H: 0.6997300386428833 Loss D_M: 0.6918027997016907\n",
            "Epoch [397/500] Loss G: 1.569283366203308 Loss D_H: 0.6973519325256348 Loss D_M: 0.6941823959350586\n",
            "Epoch [398/500] Loss G: 1.6388508081436157 Loss D_H: 0.6903223991394043 Loss D_M: 0.6882759928703308\n",
            "Epoch [399/500] Loss G: 1.6428074836730957 Loss D_H: 0.6730620861053467 Loss D_M: 0.6465208530426025\n",
            "Epoch [400/500] Loss G: 1.5763113498687744 Loss D_H: 0.6977698802947998 Loss D_M: 0.6834589838981628\n",
            "Epoch [401/500] Loss G: 1.5621230602264404 Loss D_H: 0.6951144933700562 Loss D_M: 0.6916676759719849\n",
            "Epoch [402/500] Loss G: 1.586431622505188 Loss D_H: 0.6846478581428528 Loss D_M: 0.6982190608978271\n",
            "Epoch [403/500] Loss G: 1.5506668090820312 Loss D_H: 0.6682876944541931 Loss D_M: 0.6579114198684692\n",
            "Epoch [404/500] Loss G: 1.6065583229064941 Loss D_H: 0.6969928741455078 Loss D_M: 0.6990540623664856\n",
            "Epoch [405/500] Loss G: 1.563105821609497 Loss D_H: 0.700249195098877 Loss D_M: 0.6974121332168579\n",
            "Epoch [406/500] Loss G: 1.5467337369918823 Loss D_H: 0.7107672691345215 Loss D_M: 0.7011701464653015\n",
            "Epoch [407/500] Loss G: 1.5670729875564575 Loss D_H: 0.698370635509491 Loss D_M: 0.6841697692871094\n",
            "Epoch [408/500] Loss G: 1.5759987831115723 Loss D_H: 0.6621837615966797 Loss D_M: 0.6644664406776428\n",
            "Epoch [409/500] Loss G: 1.5363008975982666 Loss D_H: 0.6901997923851013 Loss D_M: 0.697912871837616\n",
            "Epoch [410/500] Loss G: 1.56678307056427 Loss D_H: 0.6940277814865112 Loss D_M: 0.6843598484992981\n",
            "Epoch [411/500] Loss G: 1.5747233629226685 Loss D_H: 0.6935956478118896 Loss D_M: 0.6836597919464111\n",
            "Epoch [412/500] Loss G: 1.5626531839370728 Loss D_H: 0.703829288482666 Loss D_M: 0.6874037384986877\n",
            "Epoch [413/500] Loss G: 1.635665774345398 Loss D_H: 0.6690243482589722 Loss D_M: 0.6425260305404663\n",
            "Epoch [414/500] Loss G: 1.5838321447372437 Loss D_H: 0.6863322257995605 Loss D_M: 0.6969298720359802\n",
            "Epoch [415/500] Loss G: 1.5784633159637451 Loss D_H: 0.6942897439002991 Loss D_M: 0.7011337280273438\n",
            "Epoch [416/500] Loss G: 1.5609369277954102 Loss D_H: 0.6994315385818481 Loss D_M: 0.6930719614028931\n",
            "Epoch [417/500] Loss G: 1.6214202642440796 Loss D_H: 0.6945391893386841 Loss D_M: 0.6861555576324463\n",
            "Epoch [418/500] Loss G: 1.564009189605713 Loss D_H: 0.6939786076545715 Loss D_M: 0.6909298896789551\n",
            "Epoch [419/500] Loss G: 1.5893800258636475 Loss D_H: 0.6979584693908691 Loss D_M: 0.690924882888794\n",
            "Epoch [420/500] Loss G: 1.5513300895690918 Loss D_H: 0.6890265941619873 Loss D_M: 0.7071627974510193\n",
            "Epoch [421/500] Loss G: 1.555872917175293 Loss D_H: 0.6980875730514526 Loss D_M: 0.679600715637207\n",
            "Epoch [422/500] Loss G: 1.5837233066558838 Loss D_H: 0.7082735896110535 Loss D_M: 0.6743133664131165\n",
            "Epoch [423/500] Loss G: 1.607454538345337 Loss D_H: 0.6605046987533569 Loss D_M: 0.6642116904258728\n",
            "Epoch [424/500] Loss G: 1.5982136726379395 Loss D_H: 0.6782031059265137 Loss D_M: 0.6434127688407898\n",
            "Epoch [425/500] Loss G: 1.6504770517349243 Loss D_H: 0.6654592752456665 Loss D_M: 0.6440521478652954\n",
            "Epoch [426/500] Loss G: 1.5522509813308716 Loss D_H: 0.7003308534622192 Loss D_M: 0.6900410652160645\n",
            "Epoch [427/500] Loss G: 1.540960669517517 Loss D_H: 0.7018333673477173 Loss D_M: 0.6859235763549805\n",
            "Epoch [428/500] Loss G: 1.5361356735229492 Loss D_H: 0.7052886486053467 Loss D_M: 0.690438985824585\n",
            "Epoch [429/500] Loss G: 1.5740721225738525 Loss D_H: 0.7013911008834839 Loss D_M: 0.6888041496276855\n",
            "Epoch [430/500] Loss G: 1.593566656112671 Loss D_H: 0.6992542743682861 Loss D_M: 0.6822922825813293\n",
            "Epoch [431/500] Loss G: 1.582409381866455 Loss D_H: 0.6994493007659912 Loss D_M: 0.6827093362808228\n",
            "Epoch [432/500] Loss G: 1.5705876350402832 Loss D_H: 0.6920177340507507 Loss D_M: 0.6916870474815369\n",
            "Epoch [433/500] Loss G: 1.59287691116333 Loss D_H: 0.7037390470504761 Loss D_M: 0.6838200688362122\n",
            "Epoch [434/500] Loss G: 1.5344338417053223 Loss D_H: 0.690306544303894 Loss D_M: 0.692573070526123\n",
            "Epoch [435/500] Loss G: 1.5795211791992188 Loss D_H: 0.6921836137771606 Loss D_M: 0.692237138748169\n",
            "Epoch [436/500] Loss G: 1.5950920581817627 Loss D_H: 0.6802656650543213 Loss D_M: 0.7027050256729126\n",
            "Epoch [437/500] Loss G: 1.6154789924621582 Loss D_H: 0.699643611907959 Loss D_M: 0.6864495873451233\n",
            "Epoch [438/500] Loss G: 1.5841755867004395 Loss D_H: 0.6935728788375854 Loss D_M: 0.696084201335907\n",
            "Epoch [439/500] Loss G: 1.5352728366851807 Loss D_H: 0.6947481036186218 Loss D_M: 0.6934570074081421\n",
            "Epoch [440/500] Loss G: 1.6184998750686646 Loss D_H: 0.7005667686462402 Loss D_M: 0.6805676221847534\n",
            "Epoch [441/500] Loss G: 1.5908998250961304 Loss D_H: 0.6956230998039246 Loss D_M: 0.68961501121521\n",
            "Epoch [442/500] Loss G: 1.57622230052948 Loss D_H: 0.6895051002502441 Loss D_M: 0.6933289766311646\n",
            "Epoch [443/500] Loss G: 1.5655632019042969 Loss D_H: 0.6887804269790649 Loss D_M: 0.6969427466392517\n",
            "Epoch [444/500] Loss G: 1.62997567653656 Loss D_H: 0.704625129699707 Loss D_M: 0.6811907887458801\n",
            "Epoch [445/500] Loss G: 1.5069646835327148 Loss D_H: 0.694133996963501 Loss D_M: 0.7053148746490479\n",
            "Epoch [446/500] Loss G: 1.5992867946624756 Loss D_H: 0.6896019577980042 Loss D_M: 0.6920750141143799\n",
            "Epoch [447/500] Loss G: 1.5542488098144531 Loss D_H: 0.6955922842025757 Loss D_M: 0.683526873588562\n",
            "Epoch [448/500] Loss G: 1.5851397514343262 Loss D_H: 0.6846798658370972 Loss D_M: 0.6953200697898865\n",
            "Epoch [449/500] Loss G: 1.5662751197814941 Loss D_H: 0.707626223564148 Loss D_M: 0.682489275932312\n",
            "Epoch [450/500] Loss G: 1.5801805257797241 Loss D_H: 0.6966840624809265 Loss D_M: 0.6902222633361816\n",
            "Epoch [451/500] Loss G: 1.6713818311691284 Loss D_H: 0.6763805747032166 Loss D_M: 0.6416373252868652\n",
            "Epoch [452/500] Loss G: 1.5125826597213745 Loss D_H: 0.699634313583374 Loss D_M: 0.68794846534729\n",
            "Epoch [453/500] Loss G: 1.4975171089172363 Loss D_H: 0.6916080713272095 Loss D_M: 0.6916899681091309\n",
            "Epoch [454/500] Loss G: 1.6147520542144775 Loss D_H: 0.6978927850723267 Loss D_M: 0.6881303787231445\n",
            "Epoch [455/500] Loss G: 1.5343341827392578 Loss D_H: 0.6929590106010437 Loss D_M: 0.690658688545227\n",
            "Epoch [456/500] Loss G: 1.6432831287384033 Loss D_H: 0.6960984468460083 Loss D_M: 0.6842031478881836\n",
            "Epoch [457/500] Loss G: 1.5727179050445557 Loss D_H: 0.6987878084182739 Loss D_M: 0.686082124710083\n",
            "Epoch [458/500] Loss G: 1.5717309713363647 Loss D_H: 0.7000336647033691 Loss D_M: 0.6839499473571777\n",
            "Epoch [459/500] Loss G: 1.5407768487930298 Loss D_H: 0.6900594830513 Loss D_M: 0.6938112378120422\n",
            "Epoch [460/500] Loss G: 1.5718318223953247 Loss D_H: 0.7025854587554932 Loss D_M: 0.6895843744277954\n",
            "Epoch [461/500] Loss G: 1.5425113439559937 Loss D_H: 0.6886789202690125 Loss D_M: 0.6988577246665955\n",
            "Epoch [462/500] Loss G: 1.5662685632705688 Loss D_H: 0.700890302658081 Loss D_M: 0.683380126953125\n",
            "Epoch [463/500] Loss G: 1.5967522859573364 Loss D_H: 0.6942999362945557 Loss D_M: 0.6920311450958252\n",
            "Epoch [464/500] Loss G: 1.589881420135498 Loss D_H: 0.6800591945648193 Loss D_M: 0.6445704698562622\n",
            "Epoch [465/500] Loss G: 1.5976988077163696 Loss D_H: 0.6890504956245422 Loss D_M: 0.6943722367286682\n",
            "Epoch [466/500] Loss G: 1.5516772270202637 Loss D_H: 0.7010829448699951 Loss D_M: 0.6857657432556152\n",
            "Epoch [467/500] Loss G: 1.6665410995483398 Loss D_H: 0.6989559531211853 Loss D_M: 0.6891313791275024\n",
            "Epoch [468/500] Loss G: 1.5599610805511475 Loss D_H: 0.6790857315063477 Loss D_M: 0.7031761407852173\n",
            "Epoch [469/500] Loss G: 1.5832158327102661 Loss D_H: 0.6868992447853088 Loss D_M: 0.6992413997650146\n",
            "Epoch [470/500] Loss G: 1.5930415391921997 Loss D_H: 0.6911616921424866 Loss D_M: 0.6978673934936523\n",
            "Epoch [471/500] Loss G: 1.6202857494354248 Loss D_H: 0.7015299797058105 Loss D_M: 0.6823265552520752\n",
            "Epoch [472/500] Loss G: 1.627388596534729 Loss D_H: 0.7148566246032715 Loss D_M: 0.6866139769554138\n",
            "Epoch [473/500] Loss G: 1.6383870840072632 Loss D_H: 0.6966943740844727 Loss D_M: 0.6855032444000244\n",
            "Epoch [474/500] Loss G: 1.5940001010894775 Loss D_H: 0.6929622888565063 Loss D_M: 0.6933833360671997\n",
            "Epoch [475/500] Loss G: 1.562638521194458 Loss D_H: 0.6933202147483826 Loss D_M: 0.6933549642562866\n",
            "Epoch [476/500] Loss G: 1.545013427734375 Loss D_H: 0.7065945863723755 Loss D_M: 0.6816562414169312\n",
            "Epoch [477/500] Loss G: 1.5750706195831299 Loss D_H: 0.6840355396270752 Loss D_M: 0.7022390365600586\n",
            "Epoch [478/500] Loss G: 1.5570268630981445 Loss D_H: 0.7111954689025879 Loss D_M: 0.6735032796859741\n",
            "Epoch [479/500] Loss G: 1.5852961540222168 Loss D_H: 0.6960263252258301 Loss D_M: 0.691787600517273\n",
            "Epoch [480/500] Loss G: 1.5481024980545044 Loss D_H: 0.6926886439323425 Loss D_M: 0.6928087472915649\n",
            "Epoch [481/500] Loss G: 1.5149290561676025 Loss D_H: 0.6937606334686279 Loss D_M: 0.6899133324623108\n",
            "Epoch [482/500] Loss G: 1.7089383602142334 Loss D_H: 0.7085784077644348 Loss D_M: 0.6752604246139526\n",
            "Epoch [483/500] Loss G: 1.5269615650177002 Loss D_H: 0.6951326727867126 Loss D_M: 0.6933451890945435\n",
            "Epoch [484/500] Loss G: 1.6406958103179932 Loss D_H: 0.6636884212493896 Loss D_M: 0.6495493054389954\n",
            "Epoch [485/500] Loss G: 1.5417004823684692 Loss D_H: 0.68358314037323 Loss D_M: 0.6954174041748047\n",
            "Epoch [486/500] Loss G: 1.5242961645126343 Loss D_H: 0.6852767467498779 Loss D_M: 0.695038378238678\n",
            "Epoch [487/500] Loss G: 1.6226770877838135 Loss D_H: 0.6949969530105591 Loss D_M: 0.6874447464942932\n",
            "Epoch [488/500] Loss G: 1.5365815162658691 Loss D_H: 0.6892776489257812 Loss D_M: 0.6927230358123779\n",
            "Epoch [489/500] Loss G: 1.608864188194275 Loss D_H: 0.6884766817092896 Loss D_M: 0.6926730871200562\n",
            "Epoch [490/500] Loss G: 1.5568280220031738 Loss D_H: 0.6953138709068298 Loss D_M: 0.6892533302307129\n",
            "Epoch [491/500] Loss G: 1.6452314853668213 Loss D_H: 0.7028796672821045 Loss D_M: 0.6847608089447021\n",
            "Epoch [492/500] Loss G: 1.6127674579620361 Loss D_H: 0.6738802194595337 Loss D_M: 0.6421748399734497\n",
            "Epoch [493/500] Loss G: 1.6239906549453735 Loss D_H: 0.6963244676589966 Loss D_M: 0.6900359392166138\n",
            "Epoch [494/500] Loss G: 1.6582146883010864 Loss D_H: 0.689281165599823 Loss D_M: 0.6933861970901489\n",
            "Epoch [495/500] Loss G: 1.6145341396331787 Loss D_H: 0.7000750303268433 Loss D_M: 0.6886565685272217\n",
            "Epoch [496/500] Loss G: 1.6309350728988647 Loss D_H: 0.6953151226043701 Loss D_M: 0.6882903575897217\n",
            "Epoch [497/500] Loss G: 1.5361809730529785 Loss D_H: 0.6973148584365845 Loss D_M: 0.6890113949775696\n",
            "Epoch [498/500] Loss G: 1.5686776638031006 Loss D_H: 0.670190691947937 Loss D_M: 0.644951343536377\n",
            "Epoch [499/500] Loss G: 1.6128106117248535 Loss D_H: 0.697608470916748 Loss D_M: 0.6907129287719727\n",
            "Epoch [500/500] Loss G: 1.5756429433822632 Loss D_H: 0.6849960684776306 Loss D_M: 0.7008036375045776\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(G_H2M.state_dict(), 'G_H2M.pth')\n",
        "torch.save(G_M2H.state_dict(), 'G_M2H.pth')\n",
        "torch.save(D_H.state_dict(), 'D_H.pth')\n",
        "torch.save(D_M.state_dict(), 'D_M.pth')"
      ],
      "metadata": {
        "id": "lDifFJguno53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_human_dataset = SizeDataset(\"/content/test_sizes\")\n",
        "test_human_dataloader = DataLoader(test_human_dataset, batch_size=1, shuffle=False)\n",
        "G_H2M.load_state_dict(torch.load('G_H2M.pth'))\n",
        "G_H2M.eval()\n",
        "with torch.no_grad():\n",
        "    for human_sizes in test_human_dataloader:\n",
        "        generated_manga_sizes = G_H2M(human_sizes)\n",
        "        print(\"generated manga sizes:\", generated_manga_sizes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKw6bD_t5OKL",
        "outputId": "af87c0fe-9e72-4712-9ea8-b753f42128a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated manga sizes: tensor([[0.1591, 0.1532, 0.0869, 0.0911]])\n"
          ]
        }
      ]
    }
  ]
}